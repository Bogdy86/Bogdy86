####Task1#####
#IMPORT lIBRARY
from numpy.random import default_rng
rng = default_rng()
rng = default_rng()
k=''
while True:#While loop for the exception of entering only integers
    try:
      
        k=int(input('Plese enter the level of the game: '))#Input for the value of the game grid
        data = rng.uniform(1,15,(k,k))#The uniform numbers that will populate the grid
        #Creating the board
        board=[["|    |" for a in range(k)] for b in range(k)]
        for s in range (k):
            for m in range(k):
                 board[s][m]=f"  {data[s,m]:.2f}"
        print('_______'*k)
        for i in board:
            print(''.join(i))
            print('_______'*k)
        l=0
        t=0   
        h=float(board[l][t])
        h1=float(board[l][t])
        finished=False
        MODE=input('Please enter wich game mode do you want 1 or 2: ')
        print('Instructions')
        print('Up: U')
        print('Down: D')
        print('Left: L')
        print('Right: R')
        # While loop for playing the game
        while not finished: 
    
            #Condition for ending the game
            if [l,t]!=[k-1,k-1]:
                if MODE=='1':#Selecting the first game mode
                    #Chooseing the directions
                    direction=input(' Please enter one of the above options: ')
                    direction=direction.upper()
                    if direction=='U':
                            if l!=0:
                                l=l-1
                                t=t
                                h1=float(board[l][t])
                                h=h+h1
                                print(f'{h:.2f}')
                            else:
                                print("You can't go up")
            
       
        
                    elif direction=='D':
                            if l!=(k-1):
                
                                l=l+1
                                t=t
                                h1=float(board[l][t])
                                h=h+h1
                                print(f'{h:.2f}')
                            else:
                                print("You can't go down")
                
                    elif direction=='L':
                            if t!=0:
                                l=l
                                t=t-1
                                h1=float(board[l][t])
                                h=h+h1
                                print(f'{h:.2f}')
                            else:
                                print("You can't go left")
           
        
                    elif direction=='R':
                            if t!=(k-1):
                                l=l
                                t=t+1
                                h1=float(board[l][t])
                                h=h+h1
                                print(f'{h:.2f}')
                            else:
                                print("You can't go right")
                    elif  direction=='0':
                            finished=True
            
                    else:
                            print('Please press one of the fallowing U, D, L, R')
        
    
                elif MODE=='2':#Selecting the second game mood
                    #Choosing the directions
                    direction=input(' Please enter one of the above options: ')
                    direction=direction.upper()
                    if direction=='U':
                            if l!=0:
                                l=l-1
                                t=t
                                h1=float(board[l][t])
                                h=h-h1
                                h=abs(h)
                                print(f'{h:.2f}')
                            else:
                                print("You can't go up")
            
       
        
                    elif direction=='D':
                            if l!=(k-1):
                
                                l=l+1
                                t=t
                                h1=float(board[l][t])
                                h=h-h1
                                h=abs(h)
                                print(f'{h:.2f}')
                            else:
                                print("You can't go down")
                
                    elif direction=='L':
                            if t!=0:
                                l=l
                                t=t-1
                                h1=float(board[l][t])
                                h=h-h1
                                h=abs(h)
                                print(f'{h:.2f}')
                            else:
                                print("You can't go left")
           
        
                    elif direction=='R':
                            if t!=(k-1):
                                l=l
                                t=t+1
                                h1=float(board[l][t])
                                h=h-h1
                                h=abs(h)
                                print(f'{h:.2f}')
                            else:
                                print("You can't go right")
                    #Condition to enter just the four directions
                    elif  direction=='0':
                              finished=True
                
                            
                    else: 
                        print('Please press one of the fallowing U, D, L, R')
                else:
                    print('Wrong Input')
                    finished=True
                
                   
                    
            
        
                #Condition to enter just the 2 game moodes
            else:
                print(f"The path choose by you has a cost of: {h:.2f}. Next Game")    
                finished=True
                
                
    
       
            
     
    except:
        if k==0:
            break
          
        else:
            print('Only integers allowed')
###########Dijkstra altgorithm        
k=int(input('Plese enter the level of the game: '))#Input for the value of the game grid
if k==3:
    data = rng.uniform(1,15,(k,k))#The uniform numbers that will populate the grid
    #Creating the board
    board=[["|    |" for a in range(k)] for b in range(k)]
    for s in range (k):
        for m in range(k):
                board[s][m]=f"  {data[s,m]:.2f}"
    print('_______'*k)
    for i in board:
        print(''.join(i))
        print('_______'*k)
    l=0
    t=0   
    h=float(board[l][t])
    h1=float(board[l][t])


    A=float(board[0][0])
    B=float(board[1][0])
    C=float(board[2][0])
    D=float(board[0][1])
    E=float(board[1][1])
    F=float(board[2][1])
    G=float(board[0][2])
    H=float(board[1][2])
    I=float(board[2][2])



    n1 = ('A', 'B', 'C', 'D', 'E', 'F', 'G','H','I')
    distances = {
    
        'A': {'B': A+B, 'D': A+D},
        'B': {'A': A+B, 'C': B+C, 'E': B+E},
        'D': {'A': A+D, 'G': D+G, 'E': D+E},
        'G': {'D': D+G, 'H': G+H},
        'C': {'B': B+C, 'F':C+F },
        'E': {'B': B+E, 'D':D+E, 'F':E+F},
        'F': {'C': C+F, 'E': E+F, 'I':F+I},
        'H': {'E': E+H, 'G':G+H, 'I':H+I},
        'I': {'H': H+I, 'F':F+I}}

    unv = {node: None for node in n1} #using None as +inf
    v = {}
    current = 'A'
    currentDistance = 0
    unv[current] = currentDistance

    while True:
        for neighbour, distance in distances[current].items():
            if neighbour not in unv: continue
            newDistance = currentDistance + distance
            if unv[neighbour] is None or unv[neighbour] > newDistance:
                unv[neighbour] = newDistance
        v[current] = currentDistance
        del unv[current]
        if not unv: break
        candidates = [node for node in unv.items() if node[1]]
        current, currentDistance = sorted(candidates, key = lambda x: x[1])[0]
    
    

    print(f"First game mood: {v}")
    
    #########################################################################################

    n2 = ('A', 'B', 'C', 'D', 'E', 'F', 'G','H','I')
    distances2 = {
   
        'A': {'B': abs(A-B), 'D': abs(A-D)},
        'B': {'A': abs(A-B), 'C': abs(B-C), 'E': abs(B-E)},
        'D': {'A': abs(A-D), 'G': abs(D-G), 'E': abs(D-E)},
        'G': {'D': abs(D-G), 'H': abs(G-H)},
        'C': {'B': abs(B-C), 'F':abs(C-F) },
        'E': {'B': abs(B-E), 'D':abs(D-E), 'F':abs(E-F)},
        'F': {'C': abs(C-F), 'E': abs(E-F), 'I':abs(F-I)},
        'H': {'E': abs(E-H), 'G':abs(G-H), 'I':abs(H-I)},
        'I': {'H': abs(H-I), 'F':abs(F-I)}}

    unv2 = {node2: None for node2 in n2} #using None as +inf
    v2 = {}
    current2 = 'A'
    currentDistance2 = 0
    unv2[current2] = currentDistance2

    while True:
        for neighbour2, distance2 in distances2[current2].items():
            if neighbour2 not in unv2: continue
            newDistance2 = currentDistance2 + distance2
            if unv2[neighbour2] is None or unv2[neighbour2] > newDistance2:
                unv2[neighbour2] = newDistance2
        v2[current2] = currentDistance2
        del unv2[current2]
        if not unv2: break
        candidates2 = [node2 for node2 in unv2.items() if node2[1]]
        current2, currentDistance2 = sorted(candidates2, key = lambda x2: x2[1])[0]
    print(f"Second game mood: {v2}")
else:print('Please enter 3')              

   

#Task2#
from sklearn.datasets import make_regression
import matplotlib.pyplot as plt
import numpy as np
from numpy.random import default_rng
rng=default_rng()
a, b=make_regression(n_samples=300,n_features=1, noise=4, random_state=10)
outliersA = rng.normal(-4,1, (4,4))
outliersB = rng.normal(2,3, (4,4))
X=np.append(a, outliersA)
y=np.append(b, outliersB)
data=np.concatenate((X.reshape(len(X),1),y.reshape(len(y),1)),1)
plt.scatter(X, y, color='red')
plt.show
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test=train_test_split(X, y, test_size=0.25, random_state=0)
X_train=X_train[:,np.newaxis]
from sklearn.linear_model import LinearRegression
regressor=LinearRegression()
regressor.fit(X_train, y_train)
X_test=X_test[:,np.newaxis]
plt.scatter(X_test, y_test, color='red')
plt.plot(X_train, regressor.predict(X_train), color='blue')
plt.show()
plt.hist(X,bins=15, rwidth=3)
plt.show()
print('Coeficient: ',regressor.coef_)
print('Intercept: ',regressor.intercept_)
from sklearn.metrics import mean_squared_error, r2_score
print('RMSE: ',np.sqrt(mean_squared_error(y_test, y_pred)))
print('Correletion ', r2_score(y_test, y_pred))
from sklearn.metrics import mean_squared_error, r2_score
print('RMSE: ',np.sqrt(mean_squared_error(y_test, y_pred)))
print('Correletion ', r2_score(y_test, y_pred))
print('Variance of the covariance: ',X.var())
print('Standatd deviation of the covariance: ',X.std()) 
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)
upper_limit=X.mean()+2*X.std()
upper_limit
lower_limit=X.mean()-2*X.std()
lower_limit
ata[(X>upper_limit)|(X<lower_limit)]
new_data=data[(X<upper_limit)&(X>lower_limit)]
new_data
new_data.shape
X2=new_data[:,:1]
Y2=new_data[:,-1]
X2=new_data[:,:1]
Y2=new_data[:,-1]
plt.scatter(X2, Y2, color='red')
plt.show
X2_train, X2_test, y2_train, y2_test=train_test_split(X2, Y2, test_size=0.25, random_state=0)
regressor2=LinearRegression()
regressor2.fit(X2_train, y2_train)
plt.scatter(X2_train, y2_train, color='red')
plt.plot(X2_train, regressor2.predict(X2_train), color='blue')
plt.show()
plt.scatter(X2_train, y2_train, color='red')
plt.plot(X2_train, regressor2.predict(X2_train), color='blue')
plt.show()
plt.scatter(X2_test, y2_test, color='red')
plt.plot(X2_train, regressor2.predict(X2_train), color='blue')
plt.show()
print('Coeficient: ',regressor2.coef_)
print('Intercept: ',regressor2.intercept_)
print('RMSE: ',np.sqrt(mean_squared_error(y2_test, y2_pred)))
print('Correletion ', r2_score(y2_test, y2_pred))
print('Variance of the covariance: ',np.var(X2))
print('Standatd deviation of the covariance: ',np.std(X2))
plt.hist(X2,bins=15, rwidth=3)
plt.show()
X=X[:,np.newaxis]
y=y[:,np.newaxis]
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X = sc.fit_transform(X)
y= sc.transform(y)
X2_train = sc.fit_transform(X2_train)
X2_test = sc.transform(X2_test)
plt.hist(X_train,bins=15, rwidth=3)
plt.show()
plt.scatter(X,y,color='red')
plt.show()
plt.hist(X2_train,bins=15, rwidth=3)
plt.show()

*****NORMALIZATION****
#Creating the data set and the 3D graph
from mpl_toolkits.mplot3d import axes3d
%matplotlib qt
X01 = rng.normal(10000,100000, (150,1))
X11 = rng.normal(5,30, (150,1)) 
Y01=rng.normal(30,300,(150,1))
fig=plt.figure()
ax=fig.add_subplot(111,projection='3d')
ax.scatter(X01,X11,Y01, c='r', marker='o')
ax.set_xlabel('X1')
ax.set_ylabel('X2')
ax.set_zlabel('Y')
#First normalization of the data
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X02=sc.fit_transform(X01)
#3D graph after the feature scalling
from mpl_toolkits.mplot3d import axes3d
%matplotlib qt
fig=plt.figure()
ax=fig.add_subplot(111,projection='3d')
ax.scatter(X02,X11,Y01, c='r', marker='o')
ax.set_xlabel('X1')
ax.set_ylabel('X2')
ax.set_zlabel('Y')
#Graph that shows the distribution
import warnings
import seaborn as sns
warnings.filterwarnings('ignore')
plt.figure(figsize=(16,5))
plt.subplot(1,2,1)
sns.distplot(X01)
plt.subplot(1,2,2)
sns.distplot(X02)
plt.show()
#Concatenate the independent varaibles before the normalization
dataX01=np.concatenate((X01.reshape(len(X01),1), X11.reshape(len(X11),1),),1)
#Separate the test and train set
from sklearn.model_selection import train_test_split
X_train0, X_test0, y_train0, y_test0=train_test_split(dataX01, Y01, test_size=0.25, random_state=42)
#Creating the linear regression model
from sklearn.linear_model import LinearRegression
regressor0=LinearRegression()
regressor0.fit(X_train0, y_train0)
#Creating the predinctions
y_pred0=regressor0.predict(X_test0)
#Showing the coefficints, the correlation and the root mean square error
print('Coeficient: ',regressor0.coef_)
print('Intercept: ',regressor0.intercept_)
print('RMSE: ',np.sqrt(mean_squared_error(y_test0, y_pred0)))
print('Correletion ', r2_score(y_test0, y_pred0))
#Concatenate the independent varaibles after first normalization
dataX02=np.concatenate((X02.reshape(len(X02),1), X11.reshape(len(X11),1),),1)
#Separate the test and train set
from sklearn.model_selection import train_test_split
X_train02, X_test02, y_train02, y_test02=train_test_split(dataX02, Y01, test_size=0.25, random_state=42)
#Creating the linear regression model
from sklearn.linear_model import LinearRegression
regressor02=LinearRegression()
regressor02.fit(X_train02, y_train02)
#Creating the predinctions
y_pred02=regressor02.predict(X_test02)
#Showing the coefficints, the correlation and the root mean square error
print('Coeficient: ',regressor02.coef_)
print('Intercept: ',regressor02.intercept_)
print('RMSE: ',np.sqrt(mean_squared_error(y_test02, y_pred02)))
print('Correletion ', r2_score(y_test02, y_pred02))
#Second normalization of the data
from sklearn.preprocessing import MinMaxScaler
sc2=MinMaxScaler()
X03=sc2.fit_transform(X01)
#3D graph after the feature scalling
from mpl_toolkits.mplot3d import axes3d
%matplotlib qt
fig=plt.figure()
ax=fig.add_subplot(111,projection='3d')
ax.scatter(X03,X11,Y01, c='r', marker='o')
ax.set_xlabel('X1')
ax.set_ylabel('X2')
ax.set_zlabel('Y')
#Graph that shows the distribution
import warnings
import seaborn as sns
warnings.filterwarnings('ignore')
plt.figure(figsize=(16,5))
plt.subplot(1,2,1)
sns.distplot(X01)
plt.subplot(1,2,2)
sns.distplot(X03)
plt.show()
#Concatenate the independent varaibles after first normalization
dataX03=np.concatenate((X03.reshape(len(X03),1), X11.reshape(len(X11),1),),1)
#Separate the test and train set
from sklearn.model_selection import train_test_split
X_train03, X_test03, y_train03, y_test03=train_test_split(dataX03, Y01, test_size=0.25, random_state=42)
#Creating the linear regression model
from sklearn.linear_model import LinearRegression
regressor03=LinearRegression()
regressor03.fit(X_train03, y_train03)
#Creating the predinctions
y_pred03=regressor03.predict(X_test03)
#Showing the coefficints, the correlation and the root mean square error
print('Coeficient: ',regressor03.coef_)
print('Intercept: ',regressor03.intercept_)
print('RMSE: ',np.sqrt(mean_squared_error(y_test03, y_pred03)))
print('Correletion ', r2_score(y_test03, y_pred03))


#####TASK3#####
#Import libraries
import numpy as np
import pandas as pd
from matplotlib import pyplot as plt
#Load dataframe
test_data=pd.read_csv("fashion-mnist_test.csv")
train_data=pd.read_csv('fashion-mnist_train.csv')
#Separate the data
test_data = np.array(test_data)
o, p = test_data.shape
test_data = test_data.T
Y_test = test_data[0]
X_test = test_data[1:p]
X_test = X_test/ X_test.max()

train_data = np.array(train_data)
o, p = train_data.shape
train_data = train_data.T
Y_train = train_data[0]
X_train = train_data[1:p]
X_train = X_train / X_train.max()
#Set the weights and the layers
def parametres():
    weight1 = np.random.rand(20, 784)-0.5
    bias1 = np.random.rand(20, 1)-0.5
    weight1B = np.random.rand(20, 20)-0.5
    bias1B = np.random.rand(20, 1)-0.5
    weight1C = np.random.rand(20, 20)-0.5
    bias1C = np.random.rand(20, 1)-0.5
    weight2 = np.random.rand(10, 20)-0.5
    bias2 = np.random.rand(10, 1)-0.5
    return weight1, bias1,weight1B, bias1B,weight1C, bias1C, weight2, bias2
#Set the activation functions
def ReLU(L):
    return np.maximum(L, 0)

def Softmax(L):
    S = np.exp(L) / sum(np.exp(L))
    return S

def Sigmoid(L):
    SI=1.0/(1.0+np.exp(-L))
    
    return SI
#Set the forward propagation function    
def forward(weight1, bias1, weight1B, bias1B, weight1C, bias1C , weight2, bias2,  X):
    Layer1 = weight1.dot(X) + bias1
    a1 = ReLU(Layer1)
    
    Layer1B=weight1B.dot(a1)+bias1B
    a1B=ReLU(Layer1B)
    
    Layer1C=weight1C.dot(a1B)+bias1C
    a1C=ReLU(Layer1C)
    
    Layer2 = weight2.dot(a1C) + bias2
    a2 = Softmax(Layer2)
    return Layer1, a1, Layer1B, a1B,Layer1C,a1C, Layer2, a2
#The derivatives for the activations functions
def ReLUDev(L):
    return L > 0

def SigmoidDev(L):
    SID = 1.0/(1.0+np.exp(-L))*(1-(1.0/(1.0+np.exp(-L))))
    return SID
                 
#Encoding the labels       
def encoding(Y):
    encodingY = np.zeros((Y.size, Y.max() + 1))
    encodingY[np.arange(Y.size), Y] = 1
    encodingY = encodingY.T
    return encodingY
 
#Set the back propagation function
def backward(Layer1, a1,Layer1B, a1B,Layer1C,a1C, Layer2, a2, weight1,weight1B,weight1C, weight2, X, Y):
    encodingY = encoding(Y)
    dLayer2 = a2 - encodingY
    dweight2 = 1 / V * dLayer2.dot(a1C.T)
    dbias2 = 1 / V * np.sum(dLayer2)
    
    dLayer1C=weight2.T.dot(dLayer2) * ReLUDev(Layer1C)
    dweight1C = 1 / V * dLayer1C.dot(a1B.T)
    dbias1C = 1 / V * np.sum(dLayer1C)
    
    dLayer1B=weight1C.T.dot(dLayer1C) * ReLUDev(Layer1B)
    dweight1B = 1 / V * dLayer1B.dot(a1.T)
    dbias1B = 1 / V * np.sum(dLayer1B)
    
    dLayer1 = weight1B.T.dot(dLayer1B) * ReLUDev(Layer1)
    dweight1 = 1 / V * dLayer1.dot(X.T)
    dbias1 = 1 / V * np.sum(dLayer1)
    return dweight1, dbias1, dweight1B, dbias1B,dweight1C,dbias1C, dweight2, dbias2

def parametresUpdate(weight1, bias1, weight1B, bias1B,weight1C,bias1C, weight2, bias2,  dweight1, dbias1, dweight1B, dbias1B, dweight1C,dbias1C, dweight2, dbias2, beta):
    weight1 = weight1 - beta * dweight1
    bias1 = bias1 - beta * dbias1    
    
    weight2 = weight2 - beta * dweight2  
    bias2 = bias2 - beta * dbias2    
    
    weight1B = weight1B - beta * dweight1B.T
    bias1B = bias1B - beta * dbias1B 
    
    weight1C = weight1C - beta * dweight1C.T
    bias1C = bias1C - beta * dbias1C    
    return weight1, bias1, weight1B, bias1B, weight1C,bias1C, weight2, bias2 
#The prediction functions
def get_pred(a2):
    return np.argmax(a2,0)

def get_precision(pred, Y):
    print(pred, Y)
    return np.sum(pred == Y) / Y.size
import time
start_time = time.time()
#The gradient descent function
def gradientDescent(X, Y, beta, iterations):
    weight1, bias1,weight1B, bias1B, weight1C,bias1C, weight2, bias2  = parametres()
    for i in range(iterations):
        Layer1, a1, Layer1B, a1B,Layer1C, a1C, Layer2, a2 = forward( weight1, bias1,weight1B, bias1B, weight1C,bias1C, weight2, bias2, X)
        dweight1, dbias1,dweight1B, dbias1B,dweight1C,dbias1C, dweight2, dbias2  = backward(Layer1, a1,Layer1B, a1B,Layer1C,a1C, Layer2, a2,  weight1, weight1B,weight1C, weight2, X, Y)
        weight1, bias1,weight1B, bias1B,weight1C,bias1C, weight2, bias2 = parametresUpdate(weight1, bias1,weight1B, bias1B,weight1C,bias1C, weight2, bias2, dweight1, dbias1,dweight1B,dbias1B,dweight1C,dbias1C, dweight2, dbias2, beta)
        if i % 10 == 0:
            print("Epochs: ", i)
            pred = get_pred(a2)
            print(get_precision(pred, Y)*100,'%')
    print(f'\nDuration: {time.time() - start_time:.0f} seconds')
    return   weight1, bias1,weight1B, bias1B,weight1C,bias1C, weight2, bias2
#Model training 
weight1, bias1,weight1B, bias1B,weight1C,bias1C, weight2, bias2   = gradientDescent(X_train, Y_train, 0.10, 1000)
#Create the predictions for the testing set
def makePred(X, weight1, bias1,weight1B, bias1B,weight1C,bias1C, weight2, bias2):
    _, _, _, _,_,_,_,a2=forward(  weight1, bias1,weight1B, bias1B,weight1C,bias1C, weight2, bias2, X)
    pred=get_pred(a2)
    return pred
def testPred(index, weight1, bias1,weight1B, bias1B,weight1C,bias1C,weight2, bias2):
    image=X_test[:,index,None]
    pred=makePred(X_test[:, index, None],   weight1, bias1,weight1B, bias1B,weight1C,bias1C, weight2, bias2)
    label=Y_test[index]
    print('Prediction: ', pred)
    print('Label: ', label)
    image=image.reshape((28,28))
    plt.imshow(image)
    plt.show()
#Check the accuracy of the model
devPred=makePred(X_test,   weight1, bias1,weight1B, bias1B,weight1C,bias1C, weight2, bias2)
print('Precision on testing data: ',(get_precision(devPred, Y_test))*100,'%')
######################################
#####Task4#######

#Import libraries
import torch
import torch.nn as nn
import torch.nn.functional as fn 
import seaborn as sn  # for heatmaps
from torchvision import datasets, transforms
from torch.utils.data import DataLoader
from sklearn.metrics import confusion_matrix
%matplotlib inline
import matplotlib.pyplot as plt
import numpy as np
transform = transforms.ToTensor()
#Load data
data_path = "C:\Cifar10_Data"
train_Ciffar = datasets.CIFAR10(data_path, train=True, download=True, transform=transform)
test_Ciffar = datasets.CIFAR10(data_path, train=False, download=True, transform=transform)
#Separate trains and test
torch.manual_seed(101)
load_train = DataLoader(train_Ciffar, batch_size=100, shuffle=True)
load_test = DataLoader(test_Ciffar, batch_size=500, shuffle=False)
class_names = ['plane', '  car', ' bird', '  cat', ' deer', '  dog', ' frog', 'horse', ' ship', 'truck']
for images,labels in load_train:
    break
labels
#Create the layers
class Layers(nn.Module):

    def __init__(self, input_size=32*32*3, output_size=10):
        super().__init__()
        self.ciff1 = nn.Linear(input_size, 120)
        self.ciff2 = nn.Linear(120, 84)
        self.ciff3 = nn.Linear(84, output_size)
        #self.dropout = nn.Dropout(p=0.5)

    def forward(self, X):
        X = fn.relu(self.ciff1(X))
        X = fn.relu(self.ciff2(X))
        X = self.ciff3(X)

        return fn.log_softmax(X, dim=1)
 torch.manual_seed(101)
model = Layers()
model
#Count the paramers
def count_parameters(model):
    params = [p.numel() for p in model.parameters() if p.requires_grad]
    for item in params:
        print(f'{item:>6}')
    print(f'______\n{sum(params):>6}')
count_parameters(model)
#Optimizer
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(model.parameters(), lr=0.01)
#optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
images.shape
images.view(-1, 3072).shape
#Training the model
import time
start_time = time.time()

epochs = 10

train_losses = []
test_losses = []
train_correct = []
test_correct  = []

for i in range(epochs):
   
    trn_corr = 0
    tst_corr = 0
    batch_corr = 0
    
     # Run the training batches
    for b, (X_train, y_train) in enumerate(load_train):
        
        b +=1
        
        # Apply the model
        y_pred = model(X_train.view(100, -1))
        loss = criterion(y_pred, y_train)
        
         # Tally the number of correct predictions
        predicted = torch.max(y_pred.data, 1)[1]
        batch_corr = (predicted == y_train).sum()
        trn_corr += batch_corr
        
        # Update parameters
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        # Print interim results 
        if b % 100 == 0:
        
            accuracy = trn_corr.item()*100 / (100*b)
            print( f'epoch: {i} batch {b} loss:{loss.item():} accuracy:{accuracy:} % ')
    
    train_losses.append(loss)
    train_correct.append(trn_corr)

    with torch.no_grad():
      
            for b, (X_test, y_test) in enumerate(load_test):
        
                y_val = model(X_test.view(500, -1))
        
                predicted = torch.max(y_val.data, 1)[1]
                tst_corr += (predicted == y_test).sum()
     
    loss = criterion(y_val,y_test)
    test_losses.append(loss)
    test_correct.append(tst_corr)

print(f'\nDuration: {time.time() - start_time:.0f} seconds')    
#Plot the trainig validation accuracy
plt.plot([t/500 for t in train_correct], label='training accuracy')
plt.plot([t/100 for t in test_correct], label='validation accuracy')
plt.title('Accuracy at the end of each epoch')
plt.legend();
#Print the accuracy
print(test_correct) # contains the results of all 10 epochs
print()
print(f'Test accuracy: {test_correct[-1].item()*100/10000:.3f}%') # print the most recent result as a percent
# Display the number of misses
misses = np.array([])
for i in range(len(predicted.view(-1))):
    if predicted[i] != y_test[i]:
        misses = np.append(misses,i).astype('int64')
        

len(misses)
#Confusion matrix
confusion_matrix(predicted.view(-1),y_test.view(-1))
# we are turning off the gradients
img = images[0].view(1, 3072)
with torch.no_grad():
    model_prediction = model.forward(img)
#Check one image
probabilities = fn.softmax(model_prediction, dim=1).detach().cpu().numpy().squeeze()

print(probabilities)

fig, (ax1, ax2) = plt.subplots(figsize=(6,8), ncols=2)
img = img.view(3, 32, 32)
ax1.imshow(img.permute(1, 2, 0).detach().cpu().numpy().squeeze(), cmap='inferno')
ax1.axis('off')
ax2.barh(np.arange(10), probabilities, color='r' )
ax2.set_aspect(0.1)
ax2.set_yticks(np.arange(10))
ax2.set_yticklabels(np.arange(10))
ax2.set_title('Class Probability')
ax2.set_xlim(0, 1.1)

plt.tight_layout()
#Plot images and predictions
test_load_all = DataLoader(test_Ciffar, batch_size=64, shuffle=False)
images, labels = next(iter(test_load_all))

with torch.no_grad():
    images, labels = images, labels
    preds = model(X_test.view(len(X_test),-1))

images_np = [i.mean(dim=0).cpu().numpy() for i in images]
class_names = test_Ciffar.classes
fig = plt.figure(figsize=(10, 8))
fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.5, wspace=0.05)

for i in range(64):
    ax = fig.add_subplot(8, 8, i + 1, xticks=[], yticks=[])
    ax.imshow(images_np[i], cmap='gray', interpolation='nearest')
    color = "blue" if labels[i] == torch.max(preds[i], 0)[1] else "red"
    plt.title(class_names[torch.max(preds[i], 0)[1]], color=color, fontsize=15)
